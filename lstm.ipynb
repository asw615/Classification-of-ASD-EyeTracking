{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from tensorflow.keras.layers import Embedding, Flatten, LSTM, Dense, Input, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = \"../data/Eye-tracking Output/cleaned_data.csv\"\n",
    "df = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5z/ksmd9z810hv7hd63k2mmtgzr0000gn/T/ipykernel_81378/1142433817.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_relevant['Age'].fillna(0, inplace=True)\n",
      "/var/folders/5z/ksmd9z810hv7hd63k2mmtgzr0000gn/T/ipykernel_81378/1142433817.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_relevant[col] = pd.to_numeric(df_relevant[col], errors='coerce')\n",
      "/var/folders/5z/ksmd9z810hv7hd63k2mmtgzr0000gn/T/ipykernel_81378/1142433817.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_relevant[col] = df_relevant[col].fillna(method='ffill')\n",
      "/var/folders/5z/ksmd9z810hv7hd63k2mmtgzr0000gn/T/ipykernel_81378/1142433817.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_relevant[col] = df_relevant[col].fillna((df_relevant[col].shift(1) + df_relevant[col].shift(-1)) / 2)\n",
      "/var/folders/5z/ksmd9z810hv7hd63k2mmtgzr0000gn/T/ipykernel_81378/1142433817.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_relevant[col] = fill_with_row_mean(df_relevant, col)\n",
      "/var/folders/5z/ksmd9z810hv7hd63k2mmtgzr0000gn/T/ipykernel_81378/1142433817.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_relevant.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Apply the scaler to all numerical columns for this group\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     df_relevant\u001b[39m.\u001b[39mloc[group_data\u001b[39m.\u001b[39mindex, numerical_columns] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(group_data[numerical_columns])\n\u001b[1;32m     44\u001b[0m \u001b[39m# For some reason Age and CARS Score are not scaled properly, so we do it manually\u001b[39;00m\n\u001b[1;32m     45\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:508\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Scale features of X according to feature_range.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \n\u001b[1;32m    496\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39m    Transformed data.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    506\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 508\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    509\u001b[0m     X,\n\u001b[1;32m    510\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy,\n\u001b[1;32m    511\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    512\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    513\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    514\u001b[0m )\n\u001b[1;32m    516\u001b[0m X \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_\n\u001b[1;32m    517\u001b[0m X \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:774\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    768\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    769\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpandas.DataFrame with sparse columns found.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    770\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIt will be converted to a dense numpy array.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    771\u001b[0m         )\n\u001b[1;32m    773\u001b[0m dtypes_orig \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(array\u001b[39m.\u001b[39mdtypes)\n\u001b[0;32m--> 774\u001b[0m pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[1;32m    775\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[1;32m    776\u001b[0m )\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    778\u001b[0m     dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mresult_type(\u001b[39m*\u001b[39mdtypes_orig)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:775\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    768\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    769\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpandas.DataFrame with sparse columns found.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    770\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIt will be converted to a dense numpy array.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    771\u001b[0m         )\n\u001b[1;32m    773\u001b[0m dtypes_orig \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(array\u001b[39m.\u001b[39mdtypes)\n\u001b[1;32m    774\u001b[0m pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[0;32m--> 775\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[1;32m    776\u001b[0m )\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    778\u001b[0m     dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mresult_type(\u001b[39m*\u001b[39mdtypes_orig)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:614\u001b[0m, in \u001b[0;36m_pandas_dtype_needs_early_conversion\u001b[0;34m(pd_dtype)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m \u001b[39mif\u001b[39;00m is_sparse(pd_dtype) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(pd_dtype):\n\u001b[1;32m    615\u001b[0m     \u001b[39m# Sparse arrays will be converted later in `check_array`\u001b[39;00m\n\u001b[1;32m    616\u001b[0m     \u001b[39m# Only handle extension arrays for integer and floats\u001b[39;00m\n\u001b[1;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39melif\u001b[39;00m is_float_dtype(pd_dtype):\n\u001b[1;32m    619\u001b[0m     \u001b[39m# Float ndarrays can normally support nans. They need to be converted\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[39m# first to map pd.NA to np.nan\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/dtypes/common.py:234\u001b[0m, in \u001b[0;36mis_sparse\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_sparse\u001b[39m(arr) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    193\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m    Check whether an array-like is a 1-D pandas sparse array.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m    Returns `False` if the parameter has more than one dimension.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    236\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr)\n\u001b[1;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, SparseDtype)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1231\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Normalizing data ##\n",
    "\n",
    "# Feature selection of relevant columns\n",
    "relevant_columns = ['Participant', 'Point of Regard Right X [px]', 'Point of Regard Right Y [px]',\n",
    "                    'Tracking Ratio [%]', 'Category Right',\n",
    "                    'Stimulus', 'Gender', 'Age', 'Class', 'Trial', 'Pupil Diameter Right [mm]', 'Time.s']\n",
    "\n",
    "df_relevant = df[relevant_columns]\n",
    "\n",
    "# Filling NaNs in 'CARS Score' with 0\n",
    "df_relevant['Age'].fillna(0, inplace=True)\n",
    "\n",
    "# List of numerical columns to scale\n",
    "numerical_columns = ['Point of Regard Right X [px]', 'Point of Regard Right Y [px]', 'Pupil Diameter Right [mm]', 'Time.s'\n",
    "                     ] # Lidt i tvivl om vi skal have 'Age' med her. CARS og Age giver også 0. Pis lort\n",
    "\n",
    "# Convert columns to numeric, coercing errors to NaN\n",
    "for col in numerical_columns:\n",
    "    df_relevant[col] = pd.to_numeric(df_relevant[col], errors='coerce')\n",
    "\n",
    "# Define a function to fill NaN with the mean of the previous and next row\n",
    "def fill_with_row_mean(df_relevant, col):\n",
    "    # First, forward fill the first NaN (if any)\n",
    "    df_relevant[col] = df_relevant[col].fillna(method='ffill')\n",
    "    \n",
    "    # Then, fill the rest with the mean of the previous and next row\n",
    "    df_relevant[col] = df_relevant[col].fillna((df_relevant[col].shift(1) + df_relevant[col].shift(-1)) / 2)\n",
    "    \n",
    "    return df_relevant[col]\n",
    "\n",
    "# Apply this function to each numerical column\n",
    "for col in numerical_columns:\n",
    "    df_relevant[col] = fill_with_row_mean(df_relevant, col)\n",
    "\n",
    "# Handle any remaining NaNs, especially at the end of the DataFrame\n",
    "df_relevant.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Normalize data per combination of Trial, Participant, and Stimulus\n",
    "for (trial, participant, stimulus), group_data in df_relevant.groupby(['Trial', 'Participant', 'Stimulus']):\n",
    "    scaler = MinMaxScaler()\n",
    "    # Apply the scaler to all numerical columns for this group\n",
    "    df_relevant.loc[group_data.index, numerical_columns] = scaler.fit_transform(group_data[numerical_columns])\n",
    "\n",
    "# For some reason Age and CARS Score are not scaled properly, so we do it manually\n",
    "scaler = MinMaxScaler()\n",
    "df_relevant[['Age', 'Tracking Ratio [%]']] = scaler.fit_transform(df_relevant[['Age', 'Tracking Ratio [%]']])\n",
    "\n",
    "# Save the normalized data\n",
    "df_relevant.to_csv(\"../data/Eye-tracking Output/normalized_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevant = pd.read_csv(\"../data/Eye-tracking Output/normalized_data.csv\")\n",
    "# Label encoding for participant, subject, and trial. \n",
    "# Input dimensions are the number of unique values in each column and output is the square root of the input\n",
    "# embeddings = []\n",
    "# inputs = ['Stimulus', 'Trial']\n",
    "# input_dims = [114, 34]\n",
    "# output_dims = [11, 7]\n",
    "\n",
    "# for input_name, input_dim, output_dim in zip(inputs, input_dims, output_dims):\n",
    "#     le = LabelEncoder()\n",
    "#     df_relevant[input_name] = le.fit_transform(df_relevant[input_name])\n",
    "    \n",
    "#     input_layer = Input(shape=(1,))\n",
    "#     embedding_layer = Embedding(input_dim=input_dim, output_dim=output_dim)(input_layer)\n",
    "    \n",
    "#     embeddings.append(embedding_layer)\n",
    "\n",
    "# # Concatenate embeddings\n",
    "# concatenated = Concatenate()(embeddings)\n",
    "\n",
    "# # print(df_relevant['Stimulus'].unique())\n",
    "# print(concatenated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant  dtype:  float32\n",
      "Point of Regard Right X [px]  dtype:  float32\n",
      "Point of Regard Right Y [px]  dtype:  float32\n",
      "Tracking Ratio [%]  dtype:  float32\n",
      "Gender  dtype:  float32\n",
      "Age  dtype:  float32\n",
      "Class  dtype:  float32\n",
      "Trial  dtype:  object\n",
      "Pupil Diameter Right [mm]  dtype:  float32\n",
      "Time.s  dtype:  float32\n",
      "Category Right_-  dtype:  float32\n",
      "Category Right_Blink  dtype:  float32\n",
      "Category Right_Fixation  dtype:  float32\n",
      "Category Right_Saccade  dtype:  float32\n",
      "Category Right_Separator  dtype:  float32\n",
      "Stimulus_01 coucou g.jpg  dtype:  float32\n",
      "Stimulus_01 neutre3.avi  dtype:  float32\n",
      "Stimulus_01vnvg151201b1.avi  dtype:  float32\n",
      "Stimulus_02 coucou d.jpg  dtype:  float32\n",
      "Stimulus_02 devant.jpg  dtype:  float32\n",
      "Stimulus_02 neutre visage gris.jpg  dtype:  float32\n",
      "Stimulus_03 devant.jpg  dtype:  float32\n",
      "Stimulus_03 regard chien g.jpg  dtype:  float32\n",
      "Stimulus_03 vole triste vs joie1.avi  dtype:  float32\n",
      "Stimulus_04 b joie triste - copie.jpg  dtype:  float32\n",
      "Stimulus_04 regard chien d.jpg  dtype:  float32\n",
      "Stimulus_04 tete chien g.jpg  dtype:  float32\n",
      "Stimulus_05 sophie sous l'eau joie vs triste1.avi  dtype:  float32\n",
      "Stimulus_05 tete chien d.jpg  dtype:  float32\n",
      "Stimulus_05 tete point chien g.jpg  dtype:  float32\n",
      "Stimulus_06 a triste joie.jpg  dtype:  float32\n",
      "Stimulus_06 devant point chien g.jpg  dtype:  float32\n",
      "Stimulus_06 tete point chien d.jpg  dtype:  float32\n",
      "Stimulus_07 devant point chien d.jpg  dtype:  float32\n",
      "Stimulus_07 devant.jpg  dtype:  float32\n",
      "Stimulus_07 tombe joie vs triste2.avi  dtype:  float32\n",
      "Stimulus_08 b triste joie.jpg  dtype:  float32\n",
      "Stimulus_08 devant.jpg  dtype:  float32\n",
      "Stimulus_08 voc chien g.jpg  dtype:  float32\n",
      "Stimulus_09 cadeau dernier1.avi  dtype:  float32\n",
      "Stimulus_09 voc chien d.jpg  dtype:  float32\n",
      "Stimulus_09 voc devant g.jpg  dtype:  float32\n",
      "Stimulus_1 coucou D.jpg  dtype:  float32\n",
      "Stimulus_1 coucou D.png  dtype:  float32\n",
      "Stimulus_10 a joie triste.jpg  dtype:  float32\n",
      "Stimulus_10 voc devant.jpg  dtype:  float32\n",
      "Stimulus_11 devant.jpg  dtype:  float32\n",
      "Stimulus_11 punition orale triste vs joie1.avi  dtype:  float32\n",
      "Stimulus_11 yeux chat G.png  dtype:  float32\n",
      "Stimulus_11 yeux chat d.jpg  dtype:  float32\n",
      "Stimulus_11 yeux chat gauche.jpg  dtype:  float32\n",
      "Stimulus_12 a triste joie - copie.jpg  dtype:  float32\n",
      "Stimulus_12 tete chat G.png  dtype:  float32\n",
      "Stimulus_12 tete chat droite.jpg  dtype:  float32\n",
      "Stimulus_12 tete chat gauche.jpg  dtype:  float32\n",
      "Stimulus_12 yeux chat gauche.jpg  dtype:  float32\n",
      "Stimulus_13 bonbons triste vs joie1.avi  dtype:  float32\n",
      "Stimulus_13 tete chat gauche.jpg  dtype:  float32\n",
      "Stimulus_13 tete pointage chat G.png  dtype:  float32\n",
      "Stimulus_13 tete pointage chat droite.jpg  dtype:  float32\n",
      "Stimulus_13 tete pointage chat gauche.jpg  dtype:  float32\n",
      "Stimulus_14 b joie triste.jpg  dtype:  float32\n",
      "Stimulus_14 devant point chat G.png  dtype:  float32\n",
      "Stimulus_14 devant point chat droite.jpg  dtype:  float32\n",
      "Stimulus_14 devant point chat gauche.jpg  dtype:  float32\n",
      "Stimulus_14 tete pointage chat gauche.jpg  dtype:  float32\n",
      "Stimulus_15 devant - Copie.jpg  dtype:  float32\n",
      "Stimulus_15 devant point chat gauche.jpg  dtype:  float32\n",
      "Stimulus_15 devant.jpg  dtype:  float32\n",
      "Stimulus_15 devant.png  dtype:  float32\n",
      "Stimulus_16 devant.jpg  dtype:  float32\n",
      "Stimulus_16 voc chat G.png  dtype:  float32\n",
      "Stimulus_16 voc chat gauche.jpg  dtype:  float32\n",
      "Stimulus_16 voc droite chat.jpg  dtype:  float32\n",
      "Stimulus_17 voc chat gauche.jpg  dtype:  float32\n",
      "Stimulus_17 voc devant G.png  dtype:  float32\n",
      "Stimulus_17 voc devant d.jpg  dtype:  float32\n",
      "Stimulus_17 voc devant.jpg  dtype:  float32\n",
      "Stimulus_18 au revoir.jpg  dtype:  float32\n",
      "Stimulus_18 au revoir.png  dtype:  float32\n",
      "Stimulus_18 aurevoir.jpg  dtype:  float32\n",
      "Stimulus_18 voc devant.jpg  dtype:  float32\n",
      "Stimulus_19 aurevoir.jpg  dtype:  float32\n",
      "Stimulus_2 devant.jpg  dtype:  float32\n",
      "Stimulus_2 devant.png  dtype:  float32\n",
      "Stimulus_20 eye tracking (ballon droite).avi  dtype:  float32\n",
      "Stimulus_20 eye tracking (ballon gauche).avi  dtype:  float32\n",
      "Stimulus_21 neutre4.avi  dtype:  float32\n",
      "Stimulus_21 neutre5.avi  dtype:  float32\n",
      "Stimulus_22 neutre visage gris.jpg  dtype:  float32\n",
      "Stimulus_23 bonbons triste vs joie2.avi  dtype:  float32\n",
      "Stimulus_23 vole triste vs joie4.avi  dtype:  float32\n",
      "Stimulus_24 a triste joie.jpg  dtype:  float32\n",
      "Stimulus_24 b joie triste.jpg  dtype:  float32\n",
      "Stimulus_25 punition orale triste vs joie2.avi  dtype:  float32\n",
      "Stimulus_25 sophie sous l'eau joie vs triste4.avi  dtype:  float32\n",
      "Stimulus_26 a triste joie.jpg  dtype:  float32\n",
      "Stimulus_26 b joie triste.jpg  dtype:  float32\n",
      "Stimulus_27 cadeau dernier2.avi  dtype:  float32\n",
      "Stimulus_27 tombe joie vs triste5.avi  dtype:  float32\n",
      "Stimulus_28 b triste joie.jpg  dtype:  float32\n",
      "Stimulus_29 tombe joie vs triste3.avi  dtype:  float32\n",
      "Stimulus_3 regard chien D.jpg  dtype:  float32\n",
      "Stimulus_3 regard chien D.png  dtype:  float32\n",
      "Stimulus_30 a joie triste.jpg  dtype:  float32\n",
      "Stimulus_31 punition orale triste vs joie4.avi  dtype:  float32\n",
      "Stimulus_31 sophie sous l'eau joie vs triste2.avi  dtype:  float32\n",
      "Stimulus_32 b joie triste - copie.jpg  dtype:  float32\n",
      "Stimulus_33 vole triste vs joie2.avi  dtype:  float32\n",
      "Stimulus_34 a triste joie - copie.jpg  dtype:  float32\n",
      "Stimulus_4 tete chien D.jpg  dtype:  float32\n",
      "Stimulus_4 tete chien D.png  dtype:  float32\n",
      "Stimulus_5 tete point chien D.jpg  dtype:  float32\n",
      "Stimulus_5 tete point chien D.png  dtype:  float32\n",
      "Stimulus_6 devant point chien D.jpg  dtype:  float32\n",
      "Stimulus_6 devant point chien D.png  dtype:  float32\n",
      "Stimulus_7 devant - Copie.jpg  dtype:  float32\n",
      "Stimulus_7 devant.png  dtype:  float32\n",
      "Stimulus_8 voc chien D.jpg  dtype:  float32\n",
      "Stimulus_8 voc chien D.png  dtype:  float32\n",
      "Stimulus_9 voc devant D.png  dtype:  float32\n",
      "Stimulus_9 voc devant.jpg  dtype:  float32\n",
      "Stimulus_Eye Tracking (ballon droite).avi  dtype:  float32\n",
      "Stimulus_Eye Tracking (ballon gauche).avi  dtype:  float32\n",
      "Stimulus_Federica Final_WMV_3000Kbps_720p.avi  dtype:  float32\n",
      "Stimulus_NoImage  dtype:  float32\n",
      "Stimulus_VNVD151207.avi  dtype:  float32\n",
      "Stimulus_VNVG151201b.avi  dtype:  float32\n",
      "Stimulus_fede invisible d avi mpeg4-pcm.avi  dtype:  float32\n"
     ]
    }
   ],
   "source": [
    "# checking the variables to convert into dummy variables.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# for column in df_relevant.columns:\n",
    "#     print(column, \" dtype: \", df_relevant.dtypes[column])\n",
    "    # if df_relevant.dtypes[column] == \"object\":\n",
    "    #     print(column)\n",
    "\n",
    "# print(df_relevant[\"Gender\"].unique())\n",
    "# print(df_relevant[\"Category Left\"].unique())\n",
    "# print(df_relevant[\"Category Right\"].unique())\n",
    "# print(df_relevant[\"Trial\"].unique())\n",
    "# print(df_relevant[\"Stimulus\"].unique())\n",
    "\n",
    "\n",
    "#print(df_relevant[\"Gender\"].unique())\n",
    "le_gen = LabelEncoder()\n",
    "df_relevant['Gender'] = le_gen.fit_transform(df_relevant['Gender']) # M, F\n",
    "\n",
    "\n",
    "le_class = LabelEncoder()\n",
    "df_relevant['Class'] = le_class.fit_transform(df_relevant['Class']) # ASD, TD\n",
    "\n",
    "\n",
    "# Assuming 'Category' is your categorical variable in a DataFrame df\n",
    "df_encoded = pd.get_dummies(df_relevant, columns=['Category Right', 'Stimulus'], prefix=['Category Right', 'Stimulus']) # one hot encoding\n",
    "\n",
    "# print(df_encoded.iloc[:, 16:22].head(5)) # checking how it looks.\n",
    "\n",
    "for column in df_encoded.columns: #convert int64 into float64 so network it expects the same value\n",
    "    if df_encoded.dtypes[column] == \"int64\" or df_encoded.dtypes[column] == \"uint8\" or df_encoded.dtypes[column] == \"float64\":\n",
    "        df_encoded[column] = df_encoded[column].astype('float32')\n",
    "\n",
    "for column in df_encoded.columns:\n",
    "    print(column, \" dtype: \", df_encoded.dtypes[column])\n",
    "#print(df_relevant[\"Class\"].unique())\n",
    "#print(df_relevant['Participant'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tror ikke vi skal bruge den her\n",
    "\n",
    "# # Save only the normalized columns into a new DataFrame\n",
    "# normalized_columns = ['Point of Regard Right X [px]', 'Point of Regard Right Y [px]',\n",
    "#                       'Tracking Ratio [%]',\n",
    "#                       'CARS Score', 'Age']\n",
    "# df_normalized = df_relevant[normalized_columns]\n",
    "\n",
    "\n",
    "# # Combining all data into a single tensor\n",
    "# # reshaping the normalized data into 3d np array\n",
    "# normalized_np = np.stack([df_normalized[col].values for col in df_normalized.columns], 1)\n",
    "# # converting from np array to keras tensors\n",
    "# normalized_tensor = tf.convert_to_tensor(normalized_np, dtype=tf.float32)\n",
    "\n",
    "# # Add a dimension to normalized_tensor and df_encoded\n",
    "# # This transforms them from shape (905519, 7) and (905519, 19) to (905519, 1, 7) and (905519, 1, 19)\n",
    "# normalized_tensor_3d = tf.expand_dims(normalized_tensor, axis=1)\n",
    "# df_encoded_3d = tf.expand_dims(df_encoded, axis=1)\n",
    "\n",
    "# # Now you can concatenate along the last axis\n",
    "# df_all = tf.keras.layers.Concatenate(axis=-1)([normalized_tensor_3d, df_encoded_3d, concatenated])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9272, 60, 126)\n",
      "(5819, 60, 126)\n",
      "(9272, 60, 2)\n",
      "(5819, 60, 2)\n",
      "Point of Regard Right X [px] float32\n",
      "Point of Regard Right Y [px] float32\n",
      "Tracking Ratio [%] float32\n",
      "Gender float32\n",
      "Age float32\n",
      "Pupil Diameter Right [mm] float32\n",
      "Time.s float32\n",
      "Category Right_- float32\n",
      "Category Right_Blink float32\n",
      "Category Right_Fixation float32\n",
      "Category Right_Saccade float32\n",
      "Category Right_Separator float32\n",
      "Stimulus_01 coucou g.jpg float32\n",
      "Stimulus_01 neutre3.avi float32\n",
      "Stimulus_01vnvg151201b1.avi float32\n",
      "Stimulus_02 coucou d.jpg float32\n",
      "Stimulus_02 devant.jpg float32\n",
      "Stimulus_02 neutre visage gris.jpg float32\n",
      "Stimulus_03 devant.jpg float32\n",
      "Stimulus_03 regard chien g.jpg float32\n",
      "Stimulus_03 vole triste vs joie1.avi float32\n",
      "Stimulus_04 b joie triste - copie.jpg float32\n",
      "Stimulus_04 regard chien d.jpg float32\n",
      "Stimulus_04 tete chien g.jpg float32\n",
      "Stimulus_05 sophie sous l'eau joie vs triste1.avi float32\n",
      "Stimulus_05 tete chien d.jpg float32\n",
      "Stimulus_05 tete point chien g.jpg float32\n",
      "Stimulus_06 a triste joie.jpg float32\n",
      "Stimulus_06 devant point chien g.jpg float32\n",
      "Stimulus_06 tete point chien d.jpg float32\n",
      "Stimulus_07 devant point chien d.jpg float32\n",
      "Stimulus_07 devant.jpg float32\n",
      "Stimulus_07 tombe joie vs triste2.avi float32\n",
      "Stimulus_08 b triste joie.jpg float32\n",
      "Stimulus_08 devant.jpg float32\n",
      "Stimulus_08 voc chien g.jpg float32\n",
      "Stimulus_09 cadeau dernier1.avi float32\n",
      "Stimulus_09 voc chien d.jpg float32\n",
      "Stimulus_09 voc devant g.jpg float32\n",
      "Stimulus_1 coucou D.jpg float32\n",
      "Stimulus_1 coucou D.png float32\n",
      "Stimulus_10 a joie triste.jpg float32\n",
      "Stimulus_10 voc devant.jpg float32\n",
      "Stimulus_11 devant.jpg float32\n",
      "Stimulus_11 punition orale triste vs joie1.avi float32\n",
      "Stimulus_11 yeux chat G.png float32\n",
      "Stimulus_11 yeux chat d.jpg float32\n",
      "Stimulus_11 yeux chat gauche.jpg float32\n",
      "Stimulus_12 a triste joie - copie.jpg float32\n",
      "Stimulus_12 tete chat G.png float32\n",
      "Stimulus_12 tete chat droite.jpg float32\n",
      "Stimulus_12 tete chat gauche.jpg float32\n",
      "Stimulus_12 yeux chat gauche.jpg float32\n",
      "Stimulus_13 bonbons triste vs joie1.avi float32\n",
      "Stimulus_13 tete chat gauche.jpg float32\n",
      "Stimulus_13 tete pointage chat G.png float32\n",
      "Stimulus_13 tete pointage chat droite.jpg float32\n",
      "Stimulus_13 tete pointage chat gauche.jpg float32\n",
      "Stimulus_14 b joie triste.jpg float32\n",
      "Stimulus_14 devant point chat G.png float32\n",
      "Stimulus_14 devant point chat droite.jpg float32\n",
      "Stimulus_14 devant point chat gauche.jpg float32\n",
      "Stimulus_14 tete pointage chat gauche.jpg float32\n",
      "Stimulus_15 devant - Copie.jpg float32\n",
      "Stimulus_15 devant point chat gauche.jpg float32\n",
      "Stimulus_15 devant.jpg float32\n",
      "Stimulus_15 devant.png float32\n",
      "Stimulus_16 devant.jpg float32\n",
      "Stimulus_16 voc chat G.png float32\n",
      "Stimulus_16 voc chat gauche.jpg float32\n",
      "Stimulus_16 voc droite chat.jpg float32\n",
      "Stimulus_17 voc chat gauche.jpg float32\n",
      "Stimulus_17 voc devant G.png float32\n",
      "Stimulus_17 voc devant d.jpg float32\n",
      "Stimulus_17 voc devant.jpg float32\n",
      "Stimulus_18 au revoir.jpg float32\n",
      "Stimulus_18 au revoir.png float32\n",
      "Stimulus_18 aurevoir.jpg float32\n",
      "Stimulus_18 voc devant.jpg float32\n",
      "Stimulus_19 aurevoir.jpg float32\n",
      "Stimulus_2 devant.jpg float32\n",
      "Stimulus_2 devant.png float32\n",
      "Stimulus_20 eye tracking (ballon droite).avi float32\n",
      "Stimulus_20 eye tracking (ballon gauche).avi float32\n",
      "Stimulus_21 neutre4.avi float32\n",
      "Stimulus_21 neutre5.avi float32\n",
      "Stimulus_22 neutre visage gris.jpg float32\n",
      "Stimulus_23 bonbons triste vs joie2.avi float32\n",
      "Stimulus_23 vole triste vs joie4.avi float32\n",
      "Stimulus_24 a triste joie.jpg float32\n",
      "Stimulus_24 b joie triste.jpg float32\n",
      "Stimulus_25 punition orale triste vs joie2.avi float32\n",
      "Stimulus_25 sophie sous l'eau joie vs triste4.avi float32\n",
      "Stimulus_26 a triste joie.jpg float32\n",
      "Stimulus_26 b joie triste.jpg float32\n",
      "Stimulus_27 cadeau dernier2.avi float32\n",
      "Stimulus_27 tombe joie vs triste5.avi float32\n",
      "Stimulus_28 b triste joie.jpg float32\n",
      "Stimulus_29 tombe joie vs triste3.avi float32\n",
      "Stimulus_3 regard chien D.jpg float32\n",
      "Stimulus_3 regard chien D.png float32\n",
      "Stimulus_30 a joie triste.jpg float32\n",
      "Stimulus_31 punition orale triste vs joie4.avi float32\n",
      "Stimulus_31 sophie sous l'eau joie vs triste2.avi float32\n",
      "Stimulus_32 b joie triste - copie.jpg float32\n",
      "Stimulus_33 vole triste vs joie2.avi float32\n",
      "Stimulus_34 a triste joie - copie.jpg float32\n",
      "Stimulus_4 tete chien D.jpg float32\n",
      "Stimulus_4 tete chien D.png float32\n",
      "Stimulus_5 tete point chien D.jpg float32\n",
      "Stimulus_5 tete point chien D.png float32\n",
      "Stimulus_6 devant point chien D.jpg float32\n",
      "Stimulus_6 devant point chien D.png float32\n",
      "Stimulus_7 devant - Copie.jpg float32\n",
      "Stimulus_7 devant.png float32\n",
      "Stimulus_8 voc chien D.jpg float32\n",
      "Stimulus_8 voc chien D.png float32\n",
      "Stimulus_9 voc devant D.png float32\n",
      "Stimulus_9 voc devant.jpg float32\n",
      "Stimulus_Eye Tracking (ballon droite).avi float32\n",
      "Stimulus_Eye Tracking (ballon gauche).avi float32\n",
      "Stimulus_Federica Final_WMV_3000Kbps_720p.avi float32\n",
      "Stimulus_NoImage float32\n",
      "Stimulus_VNVD151207.avi float32\n",
      "Stimulus_VNVG151201b.avi float32\n",
      "Stimulus_fede invisible d avi mpeg4-pcm.avi float32\n"
     ]
    }
   ],
   "source": [
    "# train test split. infør også evt padding\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your data is loaded into a variable called 'df_encoded'\n",
    "# Modify the following line based on the actual column name of 'Class'\n",
    "\n",
    "# Extract the unique participant IDs\n",
    "participant_ids = df_encoded['Participant'].unique()\n",
    "\n",
    "# Split participant IDs into train and test sets\n",
    "train_participant_ids, test_participant_ids = train_test_split(participant_ids, test_size=0.34, random_state=3)\n",
    "\n",
    "# Filter data based on participant IDs\n",
    "train_data = df_encoded[np.isin(df_encoded['Participant'], train_participant_ids)]\n",
    "test_data = df_encoded[np.isin(df_encoded['Participant'], test_participant_ids)]\n",
    "\n",
    "test_data_participants = test_data['Participant']\n",
    "\n",
    "# Extract 'Class' column index dynamically and \n",
    "class_column_index = df_encoded.columns.get_loc('Class')\n",
    "participant_column_index = df_encoded.columns.get_loc('Participant')\n",
    "\n",
    "\n",
    "# Extract 'Class' values for train and test\n",
    "train_labels = train_data.iloc[:, class_column_index].values\n",
    "test_labels = test_data.iloc[:, class_column_index].values\n",
    "\n",
    "\n",
    "train_labels = pd.get_dummies(train_labels, columns=['Class'], prefix=['Class']) # one hot encoding\n",
    "test_labels = pd.get_dummies(test_labels, columns=['Class'], prefix=['Class']) # one hot encoding\n",
    "\n",
    "\n",
    "# Drop the 'Class' and 'Participant' column from the data\n",
    "train_data = train_data.drop(columns=['Class', 'Participant', 'Trial'])\n",
    "test_data = test_data.drop(columns=['Class', 'Participant', 'Trial'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert data into tensors with 60 samples each\n",
    "def create_tensors(data, labels, samples_per_tensor=60):\n",
    "    num_tensors = len(data) // samples_per_tensor\n",
    "    data_tensors = np.array_split(data[:num_tensors * samples_per_tensor], num_tensors)\n",
    "    labels_tensors = np.array_split(labels[:num_tensors * samples_per_tensor], num_tensors)\n",
    "    return np.stack(data_tensors), np.stack(labels_tensors) #, np.expand_dims(np.stack(labels_tensors), axis=-1)\n",
    "\n",
    "train_data_tensors, train_label_tensors = create_tensors(train_data.values, train_labels)\n",
    "test_data_tensors, test_label_tensors = create_tensors(test_data.values, test_labels)\n",
    "\n",
    "test_data_participants_tensor, lol = create_tensors(test_data_participants.values, test_labels)\n",
    "\n",
    "# checking if everythin looks good\n",
    "print(train_data_tensors.shape)\n",
    "print(test_data_tensors.shape)\n",
    "print(train_label_tensors.shape)\n",
    "print(test_label_tensors.shape)\n",
    "\n",
    "#checking up on my data before feeding it to network.\n",
    "for i in train_data.columns:\n",
    "    print(i, train_data.dtypes[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 60, 126)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(X.shape)\n",
    "\n",
    "indices = torch.randperm(len(train_data_tensors))[:200]\n",
    "\n",
    "lort = train_data_tensors[indices] # making a subset of 200\n",
    "\n",
    "lort.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "928/928 [==============================] - 13s 12ms/step - loss: 0.4041 - accuracy: 0.8150 - val_loss: 0.5341 - val_accuracy: 0.7807\n",
      "Epoch 2/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.3152 - accuracy: 0.8539 - val_loss: 0.6453 - val_accuracy: 0.7943\n",
      "Epoch 3/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.2719 - accuracy: 0.8688 - val_loss: 0.7345 - val_accuracy: 0.7969\n",
      "Epoch 4/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.2469 - accuracy: 0.8792 - val_loss: 0.8240 - val_accuracy: 0.7872\n",
      "Epoch 5/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.2271 - accuracy: 0.8883 - val_loss: 0.7210 - val_accuracy: 0.7688\n",
      "Epoch 6/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.2086 - accuracy: 0.8994 - val_loss: 1.0716 - val_accuracy: 0.7653\n",
      "Epoch 7/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.1910 - accuracy: 0.9085 - val_loss: 0.9240 - val_accuracy: 0.7801\n",
      "Epoch 8/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.1768 - accuracy: 0.9145 - val_loss: 1.2749 - val_accuracy: 0.7564\n",
      "Epoch 9/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.1757 - accuracy: 0.9181 - val_loss: 1.0405 - val_accuracy: 0.7713\n",
      "Epoch 10/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.1460 - accuracy: 0.9297 - val_loss: 1.2197 - val_accuracy: 0.7757\n",
      "Epoch 11/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.1527 - accuracy: 0.9293 - val_loss: 0.9812 - val_accuracy: 0.7701\n",
      "Epoch 12/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.1419 - accuracy: 0.9335 - val_loss: 1.4500 - val_accuracy: 0.7547\n",
      "Epoch 13/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.1427 - accuracy: 0.9313 - val_loss: 1.3664 - val_accuracy: 0.7578\n",
      "Epoch 14/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.1233 - accuracy: 0.9419 - val_loss: 1.4128 - val_accuracy: 0.7630\n",
      "Epoch 15/15\n",
      "928/928 [==============================] - 11s 12ms/step - loss: 0.1437 - accuracy: 0.9338 - val_loss: 1.3516 - val_accuracy: 0.7536\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_15 (LSTM)              (None, 60, 60)            44880     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 60, 50)            3050      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 60, 2)             102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48032 (187.62 KB)\n",
      "Trainable params: 48032 (187.62 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "# Define the LSTM model\n",
    "\n",
    "def classification_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an LSTM layer with, for example, 50 units\n",
    "    model.add(LSTM(units=60, input_shape=(60, 126), return_sequences=True))\n",
    "\n",
    "    # Add a Dense layer with the number of classes as the output dimension and activation function\n",
    "    model.add(Dense(units=50))\n",
    "\n",
    "    model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "    # Compile the model with an optimizer, loss function, and metrics\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "model = classification_model()\n",
    "\n",
    "model.fit(train_data_tensors, train_label_tensors, epochs = 15, batch_size= 10, validation_data= (test_data_tensors, test_label_tensors))\n",
    "\n",
    "\n",
    "model.save('modelus1.h5')\n",
    "\n",
    "# Display a summary of the model's architecture\n",
    "model.summary()\n",
    "\n",
    "preds = model.predict(test_data_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 6ms/step\n",
      "(5819, 60, 2)\n",
      "(5819, 60, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('modelus1.h5')\n",
    "preds = model.predict(test_data_tensors)\n",
    "\n",
    "print(preds.shape)\n",
    "print(test_label_tensors.shape)\n",
    "# Assuming preds is the output of model.predict\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming preds is the output of model.predict\n",
    "predicted_class_labels = preds.argmax(axis=-1)\n",
    "predicted_class_labels_df = pd.DataFrame({'Predicted_Class': predicted_class_labels[:, 0]}) #puts into dataframe\n",
    "\n",
    "# Print the first 50 rows\n",
    "#print(predicted_class_labels_df.head(50))\n",
    "\n",
    "true_class_labels = test_label_tensors.argmax(axis=-1)\n",
    "true_class_labels_df = pd.DataFrame({'Predicted_Class': true_class_labels[:, 0]}) #puts into dataframe\n",
    "\n",
    "#print(true_class_labels_df[0:10])\n",
    "\n",
    "\n",
    "# for i in predicted_class_labels_df['Predicted_Class'][60:100]:\n",
    "#     print(i)\n",
    "\n",
    "# for i in true_class_labels_df['Predicted_Class'][60:100]:\n",
    "#     print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "### making a prediction function based on models guesses.\n",
    "# print(preds.shape)\n",
    "dol = pd.DataFrame(test_data_participants_tensor)\n",
    "dol[\"Name\"] = dol[1]\n",
    "dol = dol.astype(object)\n",
    "\n",
    "unique_indices = dol.drop_duplicates(subset=\"Name\").index.tolist()\n",
    "\n",
    "unique_indices.append(len(dol)) #appending last index\n",
    "\n",
    "true_df_list = []\n",
    "pred_df_list = []\n",
    "\n",
    "for i in range(len(unique_indices)-1): #segment data\n",
    "    if i == unique_indices[-1]:\n",
    "        break\n",
    "    start_index = unique_indices[i]\n",
    "    end_index = unique_indices[i + 1]\n",
    "\n",
    "    # print(start_index, end_index)\n",
    "\n",
    "    true_list = true_class_labels_df[start_index: end_index]\n",
    "    pred_list = predicted_class_labels_df[start_index: end_index]\n",
    "\n",
    "    true_df_list.append(true_list)\n",
    "    pred_df_list.append(pred_list)\n",
    "\n",
    "pred_list = []\n",
    "true_list = []\n",
    "\n",
    "#now for the prediction:\n",
    "for df in pred_df_list:\n",
    "    avg = sum(df['Predicted_Class'])/len(df)\n",
    "    if avg <= 0.5:\n",
    "        pred = 1\n",
    "    elif avg > 0.5:\n",
    "        pred = 0\n",
    "    pred_list.append(pred)\n",
    "\n",
    "for df in true_df_list:\n",
    "    avg = sum(df['Predicted_Class'])/len(df)\n",
    "    if avg <= 0.5:\n",
    "        pred1 = 1\n",
    "    elif avg > 0.5:\n",
    "        pred1 = 0\n",
    "    true_list.append(pred1)\n",
    "\n",
    "\n",
    "print(pred_list)\n",
    "print(true_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5819, 60, 2)\n",
      "(5819, 60, 2)\n",
      "   0  1\n",
      "0  1  0\n",
      "1  1  0\n",
      "2  1  0\n",
      "3  1  0\n",
      "4  1  0\n",
      "[1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#making a prediction function based on sum of outputs\n",
    "\n",
    "print(preds.shape)\n",
    "print(test_label_tensors.shape)\n",
    "\n",
    "predictions = pd.DataFrame(preds[:, -1, :])\n",
    "test_label = pd.DataFrame(test_label_tensors[:, -1, :])\n",
    "\n",
    "\n",
    "print(test_label.head(5))\n",
    "\n",
    "\n",
    "True_df_list = []\n",
    "predict_df_list = []\n",
    "\n",
    "\n",
    "for i in range(len(unique_indices)-1): #segment data\n",
    "    if i == unique_indices[-1]:\n",
    "        break\n",
    "    start_index = unique_indices[i]\n",
    "    end_index = unique_indices[i + 1]\n",
    "\n",
    "    # print(start_index, end_index)\n",
    "\n",
    "    true_list = predictions[start_index: end_index]\n",
    "    pred_list = test_label[start_index: end_index]\n",
    "\n",
    "    True_df_list.append(true_list)\n",
    "    predict_df_list .append(pred_list)\n",
    "\n",
    "\n",
    "# now for predictions\n",
    "\n",
    "\n",
    "final_predictions = []\n",
    "true_list = []\n",
    "\n",
    "for i in True_df_list:\n",
    "    sum_0 = sum(i[0])\n",
    "    sum_1 = sum(i[1])\n",
    "    if sum_0 > sum_1:\n",
    "        final_predictions.append(1)\n",
    "    else:\n",
    "        final_predictions.append(0)\n",
    "\n",
    "\n",
    "for df in true_df_list:\n",
    "    avg = sum(df['Predicted_Class'])/len(df)\n",
    "    if avg <= 0.5:\n",
    "        pred1 = 1\n",
    "    elif avg > 0.5:\n",
    "        pred1 = 0\n",
    "    true_list.append(pred1)\n",
    "    \n",
    "\n",
    "print(final_predictions)\n",
    "print(true_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
